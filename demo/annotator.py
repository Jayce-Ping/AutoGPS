# Generate annotations for the images in the input dataset directory, leveraging the YOLO model and mask images (generated by a pre-trained U-Net).
# Builds a map between the labels and their corresponding geometric primitives
# This file should run after the annotation file is generated - after `inference-unet.py`


import os
import json
import re
import yaml
from ultralytics import YOLO
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import cv2
import numpy as np
import argparse
import tqdm
import torch
from itertools import combinations
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2
import segmentation_models_pytorch as smp
from transformers import TrOCRProcessor
from optimum.onnxruntime import ORTModelForVision2Seq
from scipy.optimize import linear_sum_assignment

# --- Constants ---
DEFAULT_DISTANCE_THRESHOLD_RATIO = 0.15
LINE_VALIDATION_THRESHOLD_RATIO = 0.85
YOLO_CONFIDENCE_THRESHOLD = 0.3
POINT_CLOSENESS_THRESHOLD = 30
LINE_ENDPOINT_DISTANCE_THRESHOLD = 30
PERPENDICULARITY_DOT_THRESHOLD = 1e-1
PARALLELISM_DOT_THRESHOLD = 1e-1


# --- Utility Functions ---

def is_close_point(p1, p2, threshold=POINT_CLOSENESS_THRESHOLD):
    return np.linalg.norm(np.array(p1) - np.array(p2)) < threshold

def _calculate_distance_point_to_line_segment_midpoint(point, line_p1, line_p2):
    point = np.array(point)
    line_p1 = np.array(line_p1)
    line_p2 = np.array(line_p2)

    if np.array_equal(line_p1, line_p2):
        return np.linalg.norm(point - line_p1)
    
    midpoint = (line_p1 + line_p2) / 2
    return np.linalg.norm(point - midpoint)

def _calculate_distance_point_to_line_segment(point, line_p1, line_p2):
    point = np.array(point)
    line_p1 = np.array(line_p1)
    line_p2 = np.array(line_p2)

    if np.array_equal(line_p1, line_p2):
        return np.linalg.norm(point - line_p1)
    
    line_vec = line_p2 - line_p1
    point_vec = point - line_p1
    line_len_sq = np.dot(line_vec, line_vec)
    
    t = np.dot(point_vec, line_vec) / line_len_sq
    
    if t < 0:
        closest_point_on_line = line_p1
    elif t > 1:
        closest_point_on_line = line_p2
    else:
        closest_point_on_line = line_p1 + t * line_vec

    return np.linalg.norm(point - closest_point_on_line)

def _calculate_distance_point_to_infinite_line(point, line_p1, line_p2):
    point = np.array(point)
    line_p1 = np.array(line_p1)
    line_p2 = np.array(line_p2)

    if np.array_equal(line_p1, line_p2):
        return np.linalg.norm(point - line_p1)
    
    line_unit_vector = (line_p2 - line_p1) / np.linalg.norm(line_p2 - line_p1)
    norm_vector = np.array([-line_unit_vector[1], line_unit_vector[0]])
    return np.abs(np.dot(point - line_p1, norm_vector))

def _calculate_distance_box_to_box(box1, box2):
    """
        Calculate the distance between two bounding boxes.
        Note: the distance is not the distance between the centers, but rather the minimum distance between the edges.
    """
    box1 = np.array(box1)
    box2 = np.array(box2)
    x1_center, y1_center, w1, h1 = box1
    x2_center, y2_center, w2, h2 = box2

    x1_min = x1_center - w1 / 2
    x1_max = x1_center + w1 / 2
    y1_min = y1_center - h1 / 2
    y1_max = y1_center + h1 / 2
    x2_min = x2_center - w2 / 2
    x2_max = x2_center + w2 / 2
    y2_min = y2_center - h2 / 2
    y2_max = y2_center + h2 / 2

    x_dist = max(0, x1_min - x2_max, x2_min - x1_max)
    y_dist = max(0, y1_min - y2_max, y2_min - y1_max)
    return np.sqrt(x_dist**2 + y_dist**2)


def _format_geo_element_string(element_type, points_identifiers):
    return f"{element_type}({', '.join(points_identifiers)})"


# --- Core Annotation Generation ---

def generate_annotation(yolo_model, image_path, class_to_index):
    """Generate initial annotation for a given image using the YOLO model."""
    class_names = list(class_to_index.keys())
    image_cv = cv2.imread(image_path)
    image_cv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)
    height, width = image_cv.shape[:2]

    results = yolo_model.predict(source=image_path, conf=YOLO_CONFIDENCE_THRESHOLD, verbose=False)
    image_filename = os.path.basename(image_path)
    annotation = {'image': image_filename, 'width': width, 'height': height}

    points, lines, circles, symbols = {}, {}, {}, {}
    point_cnt, symbol_cnt = 0, 0

    if not results or not results[0].boxes:
        annotation.update({'points': points, 'lines': lines, 'circles': circles, 'symbols': symbols})
        return annotation
        
    result = results[0] # Assuming one image result
    boxes_xyxy = result.boxes.xyxy.cpu().numpy()
    classes = result.boxes.cls.cpu().numpy()
    confs = result.boxes.conf.cpu().numpy()

    # Sort by class index to process points first
    sorted_detections = sorted(zip(boxes_xyxy, classes, confs), key=lambda x: x[1] != class_to_index['points'])

    for box, cls_idx, conf in sorted_detections:
        cls_idx = int(cls_idx)
        x1, y1, x2, y2 = box

        if cls_idx == class_to_index['points']:
            center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2
            point_coords = [center_x, center_y]
            if any(is_close_point(point_coords, p_coords) for p_coords in points.values()):
                continue
            point_id = f"p{point_cnt}"
            points[point_id] = [float(point_coords[0]), float(point_coords[1])]
            point_cnt += 1
        elif cls_idx == class_to_index['lines']:
            if not points: continue
            # Simplified line logic: find two closest distinct points to the line's bounding box corners
            # This part was complex and might need further refinement based on specific dataset characteristics
            # For now, we keep a simplified version of the original logic
            potential_endpoints_coords = [np.array([x1, y1]), np.array([x1, y2]), np.array([x2, y2]), np.array([x2, y1])]
            all_points_ids = list(points.keys())
            all_points_coords = list(points.values())

            min_dist_sum = float('inf')
            best_pair_ids = None

            for p_idx1, p_idx2 in combinations(range(len(all_points_ids)), 2):
                pt1_id, pt2_id = all_points_ids[p_idx1], all_points_ids[p_idx2]
                pt1_coords, pt2_coords = np.array(all_points_coords[p_idx1]), np.array(all_points_coords[p_idx2])
                
                # Check distance to bounding box corners or midpoints
                # This is a heuristic and might need adjustment
                dist_sum = 0
                for corner in potential_endpoints_coords: # Simplified check
                    dist_sum += min(np.linalg.norm(corner - pt1_coords), np.linalg.norm(corner - pt2_coords))

                if dist_sum < min_dist_sum :
                     # A more robust check would involve ensuring the line formed by pt1, pt2 aligns with the detected line segment
                    line_center_detected = np.array([(x1+x2)/2, (y1+y2)/2])
                    line_center_candidate = (pt1_coords + pt2_coords) / 2
                    if np.linalg.norm(line_center_detected - line_center_candidate) < LINE_ENDPOINT_DISTANCE_THRESHOLD * 2 : # Heuristic
                        min_dist_sum = dist_sum
                        best_pair_ids = tuple(sorted((pt1_id, pt2_id)))


            if best_pair_ids and f"{best_pair_ids[0]}-{best_pair_ids[1]}" not in lines:
                 lines[f"{best_pair_ids[0]}-{best_pair_ids[1]}"] = [points[best_pair_ids[0]], points[best_pair_ids[1]]]

        elif cls_idx == class_to_index['circles']:
            center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2
            radius = ((x2 - x1) + (y2 - y1)) / 4
            tol = max(abs(radius - abs(t)) for t in [x1 - center_x, x2 - center_x, y1 - center_y, y2 - center_y])
            center_coords = np.array([center_x, center_y])
            
            center_id = None
            min_dist_to_existing_point = float('inf')
            
            if points:
                for p_id, p_coords in points.items():
                    dist = np.linalg.norm(center_coords - np.array(p_coords))
                    if dist < min_dist_to_existing_point and is_close_point(center_coords, p_coords, threshold=radius/2): # Heuristic for closeness
                        min_dist_to_existing_point = dist
                        center_id = p_id
            
            if center_id is None:
                center_id = f"p{point_cnt}"
                points[center_id] = [float(center_coords[0]), float(center_coords[1])]
                point_cnt += 1
            circles[center_id] = [float(center_x), float(center_y), float(radius), float(tol)]
        else: # Other symbols
            center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2
            w, h = x2 - x1, y2 - y1
            symbols[f"s{symbol_cnt}"] = {
                'type': class_names[cls_idx],
                'bbox': [float(center_x), float(center_y), float(w), float(h)],
                'conf': float(conf)
            }
            symbol_cnt += 1

    annotation.update({'points': points, 'lines': lines, 'circles': circles, 'symbols': symbols})
    return annotation


def is_valid_line(mask_path_or_data, line_p1, line_p2, threshold_ratio):
    if isinstance(mask_path_or_data, str):
        mask = cv2.imread(mask_path_or_data, cv2.IMREAD_GRAYSCALE)
        if mask is None:
            raise ValueError(f"No such file: {mask_path_or_data}")
    else:
        mask = mask_path_or_data
    
    _, mask_binary = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)
    h, w = mask_binary.shape[:2]
    
    line_image = np.zeros((h, w), dtype=np.uint8)
    p1_int = tuple(np.array(line_p1, dtype=np.int32))
    p2_int = tuple(np.array(line_p2, dtype=np.int32))
    
    cv2.line(line_image, p1_int, p2_int, color=255, thickness=1) # color must be scalar for single channel
    
    line_pixels_coords = np.where(line_image == 255)
    num_line_pixels = len(line_pixels_coords[0])

    if num_line_pixels == 0:
        return False

    overlapping_pixels = np.sum(mask_binary[line_pixels_coords] == 255)
    overlap_ratio = overlapping_pixels / num_line_pixels
    
    return overlap_ratio >= threshold_ratio


def update_lines_with_mask(annotation, mask_path_or_data, threshold_ratio=LINE_VALIDATION_THRESHOLD_RATIO):
    updated_lines = {}
    if not annotation['points']:
        annotation['lines'] = updated_lines
        return annotation

    for (p1_id, p1_coords), (p2_id, p2_coords) in combinations(annotation['points'].items(), 2):
        # Ensure consistent line ID format
        sorted_ids = sorted([p1_id, p2_id])
        line_id_str = f"{sorted_ids[0]}-{sorted_ids[1]}"
        
        # Use original coordinates for validation
        original_p1_coords = annotation['points'][p1_id]
        original_p2_coords = annotation['points'][p2_id]

        if is_valid_line(mask_path_or_data, original_p1_coords, original_p2_coords, threshold_ratio):
            updated_lines[line_id_str] = [original_p1_coords, original_p2_coords]
            
    annotation['lines'] = updated_lines
    return annotation


def get_text_contents_ocr(image_path_or_pil, bboxes, ocr_processor, ocr_model):
    if isinstance(image_path_or_pil, str):
        image = Image.open(image_path_or_pil).convert("RGB")
    else:
        image = image_path_or_pil.convert("RGB")

    # Preprocessing can be crucial for OCR, consider if binary_image is always best
    gray_image = image.convert('L')
    binary_image = gray_image.point(lambda x: 0 if x < 128 else 255, '1')
    processed_image = binary_image.convert('RGB')
    # processed_image = image # Using original RGB for pix2text as it might handle color better

    cropped_images = []
    for bbox in bboxes:
        # Ensure bbox is [x_min, y_min, x_max, y_max] for PIL crop
        x_center, y_center, w, h = bbox
        x_min, y_min = x_center - w / 2, y_center - h / 2
        x_max, y_max = x_center + w / 2, y_center + h / 2
        cropped_images.append(processed_image.crop((x_min, y_min, x_max, y_max)))

    if not cropped_images:
        return []

    pixel_values = ocr_processor(images=cropped_images, return_tensors="pt").pixel_values
    generated_ids = ocr_model.generate(pixel_values)
    generated_texts = ocr_processor.batch_decode(generated_ids, skip_special_tokens=True)
    return generated_texts


def fix_text_content(symbol_type, content):
    content = content.replace(' ', '')
    content = content.replace('\\,', '')
    # Remove {} without command in front
    content = re.sub(r'\{\\bf(.+)\}', r'\1', content)  # Remove \bf{} formatting
    content = re.sub(r'\\left\((.*?)\\right\)', r'\1', content)  # Remove \left(...\right) formatting
    content = re.sub(r'\\math\w+\{(.*?)\}', r'\1', content)
    content = re.sub(r'\\boldsymbol\{(.*?)\}', r'\1', content)
    content = re.sub(r'\^{(.*?)}', r'\1', content)  # Remove ^{} formatting

    content = re.sub(r'(?:^|(?<=[^\\a-zA-Z^_]))\{([^{}]*)\}', r'\1', content)
    content = re.sub(r'\{\}', '', content)

    if symbol_type == 'text-len':
        # Replace all \\sqrt to 'sqrt'
        content = re.sub(r'\\sqrt\{(.*?)\}', r'sqrt{\1}', content)
        # Check if there is a backslash in the content
        match_backslash = re.search(r'\\', content)
        if match_backslash:
            content = content[:match_backslash.start()]

        # Replace sqrt back to \\sqrt
        content = re.sub(r'sqrt\{(.*?)\}', r'\\sqrt{\1}', content)

        # Remove parentheses if they are around the content
        if content.startswith('(') and content.endswith(')'):
            content = content[1:-1]

        # Remove some metric units
        # in. cm. m. ft. yd. mi. and without .
        content = re.sub(r'\b(in|cm|m|ft|yd|mi)\.?', '', content)
    elif symbol_type == 'text-degree':
        content = content.replace('\\circ', '')
        # Remove parentheses if they are around the content
        if content.startswith('(') and content.endswith(')'):
            content = content[1:-1]
    elif symbol_type == 'text-point':
        content = content.upper() if len(content) == 1 and content.isalpha() else content
        content = 'E' if content == '\\epsilon' else content

    elif symbol_type == 'text-area':
        content = content[2:] if content.startswith('A=') else content
        # Remove parentheses if they are around the content
        if content.startswith('(') and content.endswith(')'):
            content = content[1:-1]
    elif symbol_type == 'text-angle': # Specific OCR error corrections
        content = re.sub(r'rceil', '1', content)
        content = re.sub(r'lambda', '2', content)
    
    return content


def update_text_content_with_ocr(annotation, image_path, ocr_processor, ocr_model):
    text_symbols_data = [(sid, sdata) for sid, sdata in annotation['symbols'].items() if 'text' in sdata['type']]
    if not text_symbols_data:
        return annotation

    selected_sym_ids = [item[0] for item in text_symbols_data]
    bboxes = [item[1]['bbox'] for item in text_symbols_data]
    
    texts = get_text_contents_ocr(image_path, bboxes, ocr_processor, ocr_model)

    for i, sym_id in enumerate(selected_sym_ids):
        if i < len(texts):
            raw_content = texts[i]
            annotation['symbols'][sym_id]['content'] = raw_content
            annotation['symbols'][sym_id]['fixed_content'] = fix_text_content(annotation['symbols'][sym_id]['type'], raw_content)
    return annotation

# --- Symbol Mapping ---
def _get_effective_symbol_data(sym_id, base_data_map, arrow_map_inv, arrow_head_map, is_point=False):
    """Resolves symbol location if affected by an arrow."""
    eff_sym_id = sym_id
    if sym_id in arrow_map_inv:
        arrow_id = arrow_map_inv[sym_id]
        if arrow_id in arrow_head_map:
            eff_sym_id = arrow_head_map[arrow_id]
    
    data = base_data_map.get(eff_sym_id)
    if data is None: return None # Should not happen if maps are consistent

    if is_point: # Data is [x,y]
        return np.array(data)
    else: # Data is bbox [cx, cy, w, h]
        return np.array(data)


def _match_elements(source_elements, target_elements, cost_func, threshold):
    """Generic element matching using Hungarian algorithm."""
    if not source_elements or not target_elements:
        return {}

    source_ids, source_data = zip(*source_elements.items())
    target_ids, target_data = zip(*target_elements.items())

    cost_matrix = np.full((len(source_ids), len(target_ids)), float('inf'))
    for i, s_id in enumerate(source_ids):
        for j, t_id in enumerate(target_ids):
            cost_matrix[i, j] = cost_func(s_id, source_data[i], t_id, target_data[j])

    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    
    mapping = {}
    for r, c in zip(row_ind, col_ind):
        if cost_matrix[r, c] < threshold:
            mapping[target_ids[c]] = source_ids[r] # target_id -> source_id
    return mapping


def build_symbol_map(annotation, distance_threshold_ratio=DEFAULT_DISTANCE_THRESHOLD_RATIO):
    img_width, img_height = annotation['width'], annotation['height']
    abs_dist_threshold = distance_threshold_ratio * max(img_width, img_height)

    symbols = annotation['symbols']
    points = annotation['points']
    lines = annotation['lines']

    # Prepare base data maps (original locations)
    # Bbox format: [center_x, center_y, width, height]
    # Point format: [x, y]
    sym_id_to_bbox_orig = {sid: np.array(sinfo['bbox']) for sid, sinfo in symbols.items()}
    # For points, store as [x,y,0,0] to somewhat fit bbox logic if needed, or handle separately
    point_id_to_coords_orig = {pid: np.array(pinfo) for pid, pinfo in points.items()}


    # 1. Arrow mapping (simplified from original, may need more robust logic for head finding)
    arrow_map = {} # arrow_id -> metric_symbol_id (e.g. text-len, text-degree)
    arrow_map_inverse = {}
    arrow_to_head_map = {} # arrow_id -> head_symbol_id (if 'head' symbol is found near arrow)

    arrows = {sid: sinfo for sid, sinfo in symbols.items() if 'arrow' in sinfo['type']}
    metric_symbols_for_arrows = {sid: sinfo for sid, sinfo in symbols.items() if sinfo['type'] in ['text-len', 'text-degree']}
    head_symbols = {sid: sinfo for sid, sinfo in symbols.items() if 'head' in sinfo['type']}

    for arr_id, arr_info in arrows.items():
        arr_center = np.array(arr_info['bbox'][:2])
        arr_bbox = np.array(arr_info['bbox'])
        # Find closest metric symbol
        if metric_symbols_for_arrows:
            closest_metric_sym_id = min(
                metric_symbols_for_arrows.keys(),
                # key=lambda msid: np.linalg.norm(arr_center - np.array(metric_symbols_for_arrows[msid]['bbox'][:2]))
                key= lambda msid: _calculate_distance_box_to_box(arr_bbox, np.array(metric_symbols_for_arrows[msid]['bbox']))
            )
            arrow_map[arr_id] = closest_metric_sym_id
            arrow_map_inverse[closest_metric_sym_id] = arr_id

        # Find associated head (simple proximity to arrow center)
        if head_symbols:
            # This logic for head association is very basic and might need improvement
            # e.g., check if head is contained within or very near the arrow's bounding box
            # and potentially consider arrow direction if available.
            closest_head_id = min(
                head_symbols.keys(),
                key=lambda hid: _calculate_distance_box_to_box(arr_bbox, np.array(head_symbols[hid]['bbox']))
            )
            # Add a distance check for the head
            if np.linalg.norm(arr_center - np.array(head_symbols[closest_head_id]['bbox'][:2])) < abs_dist_threshold: # Heuristic
                 arrow_to_head_map[arr_id] = closest_head_id


    # Effective data getters
    def get_eff_point_coords(p_id):
        return _get_effective_symbol_data(p_id, point_id_to_coords_orig, arrow_map_inverse, arrow_to_head_map, is_point=True)

    def get_eff_symbol_bbox(s_id): # For text symbols, etc.
        return _get_effective_symbol_data(s_id, sym_id_to_bbox_orig, arrow_map_inverse, arrow_to_head_map, is_point=False)


    # Cost functions for matching
    def point_text_cost(text_s_id, text_s_data, p_id, p_data):
        text_center = get_eff_symbol_bbox(text_s_id)[:2] # text_s_data is original bbox
        point_coords = get_eff_point_coords(p_id) # p_data is original coords
        if text_center is None or point_coords is None: return float('inf')
        return np.linalg.norm(text_center - point_coords)

    def line_text_cost(text_s_id, text_s_data, line_id, line_data_coords): # line_data_coords are [[x1,y1],[x2,y2]]
        text_bbox = get_eff_symbol_bbox(text_s_id)
        if text_bbox is None: return float('inf')
        text_center = text_bbox[:2]
        # For lines, use original point coordinates that form the line
        p1_id, p2_id = line_id.split('-')
        p1_coords, p2_coords = annotation['points'][p1_id], annotation['points'][p2_id]
        # return _calculate_distance_point_to_line_segment(text_center, p1_coords, p2_coords)
        return _calculate_distance_point_to_line_segment_midpoint(text_center, p1_coords, p2_coords)


    def angle_text_cost(text_s_id, text_s_data, angle_id_str, angle_points_ids):
        # angle_points_ids: (p_adj1_id, p_vertex_id, p_adj2_id)
        text_bbox = get_eff_symbol_bbox(text_s_id)
        if text_bbox is None: return float('inf')
        text_center = text_bbox[:2]

        p_adj1_coords = get_eff_point_coords(angle_points_ids[0])
        p_vertex_coords = get_eff_point_coords(angle_points_ids[1])
        p_adj2_coords = get_eff_point_coords(angle_points_ids[2])

        if any(c is None for c in [p_adj1_coords, p_vertex_coords, p_adj2_coords]): return float('inf')

        # Check if text_center is within the angle using cross products
        vec1 = p_adj1_coords - p_vertex_coords
        vec2 = p_adj2_coords - p_vertex_coords
        vec_text = text_center - p_vertex_coords
        # Extend to 3D vectors
        vec1 = np.append(vec1, 0)
        vec2 = np.append(vec2, 0)
        vec_text = np.append(vec_text, 0)
        
        cross_prod1 = np.cross(vec1, vec_text)
        cross_prod2 = np.cross(vec_text, vec2)
        # Get the sign of the z-component of the cross products
        cross_prod1 = cross_prod1[2]
        cross_prod2 = cross_prod2[2]

        if cross_prod1 * cross_prod2 >= 0: # Same sign means inside or on the boundary
            # Average distance to the two arms (segments)
            dist1 = _calculate_distance_point_to_line_segment(text_center, p_vertex_coords, p_adj1_coords)
            dist2 = _calculate_distance_point_to_line_segment(text_center, p_vertex_coords, p_adj2_coords)
            return (dist1 + dist2) / 2
        return float('inf')


    # --- Perform matching ---
    res = {'point': {}, 'line': {}, 'angle': {}, 'angle-equality': {}, 'angle-index': {}}
    
    # 1. Match text-point to points
    text_points_syms = {sid: sinfo for sid, sinfo in symbols.items() if sinfo['type'] == 'text-point'}
    res['point'] = _match_elements(text_points_syms, points, point_text_cost, abs_dist_threshold)
    
    # 2. Match text-len to lines
    text_len_syms = {sid: sinfo for sid, sinfo in symbols.items() if sinfo['type'] == 'text-len'}
    # Consider only lines whose endpoints are already mapped to text-points (explicitly labeled points)
    # This was a heuristic in the original code. For a more general approach, all lines could be considered.
    # For now, let's use all lines from annotation['lines']
    res['line'] = _match_elements(text_len_syms, lines, line_text_cost, abs_dist_threshold)

    # 3. Match angle texts to angles
    # Generate angle candidates: (adj1_id, vertex_id, adj2_id)
    angle_candidates = {} # "p1-p0-p2" -> (p1_id, p0_id, p2_id)
    # Only consider points that are part of at least two lines for forming angles
    point_connectivity = {pid: [] for pid in points}
    for line_id_str in lines:
        p1_id, p2_id = line_id_str.split('-')
        point_connectivity[p1_id].append(p2_id)
        point_connectivity[p2_id].append(p1_id)

    for v_id, connected_p_ids in point_connectivity.items():
        if len(connected_p_ids) >= 2:
            for p1_adj_id, p2_adj_id in combinations(connected_p_ids, 2):
                # Ensure points are distinct and form a non-degenerate angle
                # (simple check: points are not collinear through the vertex in a trivial way)
                angle_key_parts = sorted([p1_adj_id, p2_adj_id])
                # A more robust check would involve vector math.
                v1 = np.array(points[p1_adj_id]) - np.array(points[v_id])
                v2 = np.array(points[p2_adj_id]) - np.array(points[v_id])
                v1_norm = v1 / np.linalg.norm(v1)
                v2_norm = v2 / np.linalg.norm(v2)
                d_pro = np.dot(v1, v2)
                if abs(abs(np.dot(v1_norm, v2_norm)) - 1) < PARALLELISM_DOT_THRESHOLD: # Check for collinearity (parallel)
                    continue

                angle_id_str = f"{angle_key_parts[0]}-{v_id}-{angle_key_parts[1]}"
                angle_candidates[angle_id_str] = (angle_key_parts[0], v_id, angle_key_parts[1])

    
    angle_text_map = {
        'angle': {sid: sinfo for sid, sinfo in symbols.items() if sinfo['type'] == 'text-degree'},
        'angle-equality': {sid: sinfo for sid, sinfo in symbols.items() if 'angle-others' in sinfo['type']}, # e.g. single/double angle marks
        'angle-index': {sid: sinfo for sid, sinfo in symbols.items() if sinfo['type'] == 'text-angle'} # e.g. angle 1, angle 2
    }

    for angle_type_key, text_angle_syms_dict in angle_text_map.items():
        if angle_candidates and text_angle_syms_dict:
             res[angle_type_key] = _match_elements(text_angle_syms_dict, angle_candidates, angle_text_cost, abs_dist_threshold)
    
    return res


def annotate_keypoints(annotation, symbol_maps):
    symbols_dict = annotation['symbols']
    points_dict = annotation['points']
    
    point_id_to_identifier = {}
    # Mapped points
    for point_id, text_point_sym_id in symbol_maps.get('point', {}).items():
        if text_point_sym_id in symbols_dict and 'fixed_content' in symbols_dict[text_point_sym_id]:
            point_id_to_identifier[point_id] = symbols_dict[text_point_sym_id]['fixed_content']
        elif text_point_sym_id in symbols_dict and 'content' in symbols_dict[text_point_sym_id]: # Fallback
             point_id_to_identifier[point_id] = symbols_dict[text_point_sym_id]['content']


    # Unlabeled points
    unlabeled_point_ids = [pid for pid in points_dict if pid not in point_id_to_identifier]
    if unlabeled_point_ids:
        used_letters = set(val for val in point_id_to_identifier.values() if len(val)==1 and val.isalpha())
        available_letters = sorted([chr(i) for i in range(ord('A'), ord('Z') + 1) if chr(i) not in used_letters])
        
        for i, point_id in enumerate(unlabeled_point_ids):
            if i < len(available_letters):
                point_id_to_identifier[point_id] = available_letters[i]
            else: # Fallback if not enough letters
                point_id_to_identifier[point_id] = f"P{i+len(available_letters)}" 
    return point_id_to_identifier


# --- Geometric Relation Solvers ---

def solve_point_on_line_relation(annotation, symbol_maps, point_id_to_identifier, on_line_threshold=POINT_CLOSENESS_THRESHOLD):
    """
        Solve point-on-line relations based on the annotation and symbol maps.
        If one point is close enough to the line segment and is not one of the endpoint, it is considered "on" the line.
    """
    points_dict = annotation['points']
    lines_dict = annotation['lines']
    
    point_on_line_relations = []

    for p_id, p_coords_list in points_dict.items():
        p_coords = np.array(p_coords_list)
        p_identifier = point_id_to_identifier.get(p_id)

        if p_identifier is None:
            continue # Skip if point has no identifier

        for line_id_str, line_endpoints_coords_pair in lines_dict.items():
            line_p1_coords_list, line_p2_coords_list = line_endpoints_coords_pair
            line_p1_coords = np.array(line_p1_coords_list)
            line_p2_coords = np.array(line_p2_coords_list)

            # Get endpoint IDs of the current line
            ep_id1, ep_id2 = line_id_str.split('-')

            # A point is not "on" the line if it is one of the line's endpoints
            if p_id == ep_id1 or p_id == ep_id2:
                continue

            distance = _calculate_distance_point_to_line_segment(p_coords, line_p1_coords, line_p2_coords)

            if distance < on_line_threshold:
                ep1_identifier = point_id_to_identifier.get(ep_id1)
                ep2_identifier = point_id_to_identifier.get(ep_id2)

                if ep1_identifier and ep2_identifier:
                    # Ensure consistent order for line identifiers
                    line_key = tuple(sorted((ep1_identifier, ep2_identifier)))
                    point_on_line_relations.append((p_identifier, line_key))
                    # If a point can be on multiple lines, this list will capture all such instances.
                    
    return {'point_on_line': point_on_line_relations}


def solve_point_on_circle_relation(annotation, symbol_maps, point_id_to_identifier, on_circle_threshold=POINT_CLOSENESS_THRESHOLD):
    """
        Solve point-on-circle relations based on the annotation and symbol maps.
        A point is considered "on" the circle if it is within a certain threshold distance from the circle's circumference.
    """
    points_dict = annotation['points']
    circles_dict = annotation['circles']
    
    point_on_circle_relations = []

    for p_id, p_coords_list in points_dict.items():
        p_coords = np.array(p_coords_list)
        p_identifier = point_id_to_identifier.get(p_id)

        if p_identifier is None:
            continue # Skip if point has no identifier

        for idx, (c_id, c_data) in enumerate(circles_dict.items()):
            c_center = np.array(c_data[:2])  # [cx, cy, radius, tolerance]
            radius = c_data[2]
            tolerance = c_data[3]

            c_identifier = point_id_to_identifier.get(c_id)
            radius_identifier = f"r_{idx}"

            distance = np.linalg.norm(p_coords - c_center)

            if abs(distance - radius) < on_circle_threshold + tolerance:
                point_on_circle_relations.append((p_identifier, (c_identifier, radius_identifier)))

    return {'point_on_circle': point_on_circle_relations}

def solve_assignment_correspondence(annotation, symbol_maps, point_id_to_identifier):
    symbols_dict = annotation['symbols']
    relations = {'line_to_length': {}, 'angle_to_degree': {}, 'shape_to_area': {}}

    for line_id, len_sym_id in symbol_maps.get('line', {}).items():
        p1_id, p2_id = line_id.split('-')
        if p1_id in point_id_to_identifier and p2_id in point_id_to_identifier and \
           len_sym_id in symbols_dict and 'fixed_content' in symbols_dict[len_sym_id]:
            p1_ident, p2_ident = point_id_to_identifier[p1_id], point_id_to_identifier[p2_id]
            # Ensure consistent order for line identifiers
            line_key = tuple(sorted((p1_ident, p2_ident)))
            relations['line_to_length'][line_key] = symbols_dict[len_sym_id]['fixed_content']

    for angle_id_str, deg_sym_id in symbol_maps.get('angle', {}).items():
        # angle_id_str is "p_adj1-p_vertex-p_adj2"
        p_adj1_id, p_vertex_id, p_adj2_id = angle_id_str.split('-')
        if all(pid in point_id_to_identifier for pid in [p_adj1_id, p_vertex_id, p_adj2_id]) and \
           deg_sym_id in symbols_dict and 'fixed_content' in symbols_dict[deg_sym_id]:
            p_adj1_ident = point_id_to_identifier[p_adj1_id]
            p_vertex_ident = point_id_to_identifier[p_vertex_id]
            p_adj2_ident = point_id_to_identifier[p_adj2_id]
            relations['angle_to_degree'][(p_adj1_ident, p_vertex_ident, p_adj2_ident)] = symbols_dict[deg_sym_id]['fixed_content']
    
    area_cnt = 0
    for sym_id, sym_info in symbols_dict.items():
        if sym_info['type'] == 'text-area' and 'fixed_content' in sym_info:
            relations['shape_to_area'][f"Shape({area_cnt})"] = sym_info['fixed_content']
            area_cnt += 1
    return relations


def solve_angle_equality(annotation, symbol_maps, point_id_to_identifier):
    symbols_dict = annotation['symbols']
    angle_equality_map = symbol_maps.get('angle-equality', {})
    
    groups_by_sym_type = {} # e.g. "angle-others-single" -> list of angle_ident_tuples
    for angle_id_str, sym_id in angle_equality_map.items():
        p_adj1_id, p_vertex_id, p_adj2_id = angle_id_str.split('-')
        if all(pid in point_id_to_identifier for pid in [p_adj1_id, p_vertex_id, p_adj2_id]) and \
           sym_id in symbols_dict:
            sym_type = symbols_dict[sym_id]['type']
            p_adj1_ident = point_id_to_identifier[p_adj1_id]
            p_vertex_ident = point_id_to_identifier[p_vertex_id]
            p_adj2_ident = point_id_to_identifier[p_adj2_id]
            
            angle_tuple = (p_adj1_ident, p_vertex_ident, p_adj2_ident)
            groups_by_sym_type.setdefault(sym_type, []).append(angle_tuple)
            
    equations = []
    for angles_list in groups_by_sym_type.values():
        for angle1, angle2 in combinations(angles_list, 2):
            equations.append((angle1, angle2))
    return {'angle_equations': equations}


def _find_closest_lines_for_bar_symbols(bar_symbols, lines_dict, point_id_to_identifier, img_width, img_height, dist_thresh_ratio):
    group = []
    abs_dist_thresh = dist_thresh_ratio * max(img_width, img_height)
    if not lines_dict: return group

    for bar_id, bar_info in bar_symbols.items():
        bar_center = np.array(bar_info['bbox'][:2])
        closest_line_id_str, min_dist = None, float('inf')

        for line_id_str, line_coords_pair in lines_dict.items():
            dist = _calculate_distance_point_to_line_segment(bar_center, line_coords_pair[0], line_coords_pair[1])
            if dist < min_dist:
                min_dist = dist
                closest_line_id_str = line_id_str
        
        if closest_line_id_str and min_dist < abs_dist_thresh:
            p1_id, p2_id = closest_line_id_str.split('-')
            if p1_id in point_id_to_identifier and p2_id in point_id_to_identifier:
                p1_ident, p2_ident = point_id_to_identifier[p1_id], point_id_to_identifier[p2_id]
                group.append(tuple(sorted((p1_ident, p2_ident))))
    return group


def solve_length_equality(annotation, symbol_maps, point_id_to_identifier):
    symbols_dict = annotation['symbols']
    lines_dict = annotation['lines']
    w, h = annotation['width'], annotation['height']

    bars = {sid: sinfo for sid, sinfo in symbols_dict.items() if sinfo['type'] == "bar-others"}
    double_bars = {sid: sinfo for sid, sinfo in symbols_dict.items() if sinfo['type'] == "double bar-others"}

    group1 = _find_closest_lines_for_bar_symbols(bars, lines_dict, point_id_to_identifier, w, h, DEFAULT_DISTANCE_THRESHOLD_RATIO)
    group2 = _find_closest_lines_for_bar_symbols(double_bars, lines_dict, point_id_to_identifier, w, h, DEFAULT_DISTANCE_THRESHOLD_RATIO)
    
    equations = []
    for group in [group1, group2]:
        if len(group) > 1:
            for l1, l2 in combinations(group, 2):
                equations.append((l1, l2))
    return {'length_equations': equations}


def solve_perpendicularity(annotation, symbol_maps, point_id_to_identifier):
    img_width, img_height = annotation['width'], annotation['height']
    symbols_dict = annotation['symbols']
    lines_dict = annotation['lines']
    if len(lines_dict) < 2: return {'perpendicularity': []}

    perp_symbols = {sid: sinfo for sid, sinfo in symbols_dict.items() if 'perpendicular' in sinfo['type']}
    perpendicular_pairs = []

    for perp_sym_id, perp_info in perp_symbols.items():
        perp_center = np.array(perp_info['bbox'][:2])
        
        # Find two lines closest to the perpendicular symbol, whose intersection is also close
        # This is a heuristic. A more robust method might involve checking all pairs of intersecting lines.

        def combined_dist(line1_coords, line2_coords):
            dist_to_l1 = _calculate_distance_point_to_line_segment(perp_center, line1_coords[0], line1_coords[1])
            dist_to_l2 = _calculate_distance_point_to_line_segment(perp_center, line2_coords[0], line2_coords[1])
            return dist_to_l1 + dist_to_l2
        
        def perpendicular_check(l1_id, l2_id):
            l1_coords = lines_dict[l1_id]
            l2_coords = lines_dict[l2_id]
            v1 = np.array(l1_coords[1]) - np.array(l1_coords[0])
            v2 = np.array(l2_coords[1]) - np.array(l2_coords[0])
            if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0: return False
            v1_norm, v2_norm = v1 / np.linalg.norm(v1), v2 / np.linalg.norm(v2)
            return abs(np.dot(v1_norm, v2_norm)) < PERPENDICULARITY_DOT_THRESHOLD

        
        sorted_line_pairs = sorted(
            [(l1_id_str, l2_id_str, combined_dist(lines_dict[l1_id_str], lines_dict[l2_id_str])) for l1_id_str, l2_id_str in combinations(lines_dict.keys(), 2) if perpendicular_check(l1_id_str, l2_id_str)],
            key=lambda t: t[2]  # Sort by combined distance
        )
        for l1, l2, dis in sorted_line_pairs:
            if dis > DEFAULT_DISTANCE_THRESHOLD_RATIO * (img_width + img_height):
                continue
            l1_p1_id, l1_p2_id = l1.split('-')
            l2_p1_id, l2_p2_id = l2.split('-')
            if all(pid in point_id_to_identifier for pid in [l1_p1_id, l1_p2_id, l2_p1_id, l2_p2_id]):
                l1_p1_ident, l1_p2_ident = point_id_to_identifier[l1_p1_id], point_id_to_identifier[l1_p2_id]
                l2_p1_ident, l2_p2_ident = point_id_to_identifier[l2_p1_id], point_id_to_identifier[l2_p2_id]
                canonical_pair = tuple(sorted((l1_p1_ident, l1_p2_ident, l2_p1_ident, l2_p2_ident)))
                if canonical_pair not in perpendicular_pairs:
                    perpendicular_pairs.append((tuple(sorted((l1_p1_ident, l1_p2_ident))), tuple(sorted((l2_p1_ident, l2_p2_ident)))))
                    break
    
    return {'perpendicularity': list(set(perpendicular_pairs))} # Ensure uniqueness


def solve_parallelism(annotation, symbol_maps, point_id_to_identifier):
    symbols_dict = annotation['symbols']
    lines_dict = annotation['lines']
    w, h = annotation['width'], annotation['height']
    if len(lines_dict) < 2: return {'parallelism': []}

    parallel_symbols = {sid: sinfo for sid, sinfo in symbols_dict.items() if 'parallel' in sinfo['type']}
    # Associate each parallel symbol with its closest line
    symbol_to_closest_line = {}
    abs_dist_thresh = DEFAULT_DISTANCE_THRESHOLD_RATIO * max(w,h)

    for par_sym_id, par_info in parallel_symbols.items():
        par_center = np.array(par_info['bbox'][:2])
        closest_line_id_str, min_dist = None, float('inf')
        for line_id_str, line_coords_pair in lines_dict.items():
            dist = _calculate_distance_point_to_line_segment(par_center, line_coords_pair[0], line_coords_pair[1])
            if dist < min_dist:
                min_dist = dist
                closest_line_id_str = line_id_str
        if closest_line_id_str and min_dist < abs_dist_thresh:
            symbol_to_closest_line.setdefault(par_sym_id, []).append(closest_line_id_str) # Could be multiple symbols per line

    # Group lines that share the same type of parallel marking (heuristic: all parallel symbols imply one group of parallel lines)
    # A more robust system might differentiate types of parallel marks (e.g. single arrow, double arrow)
    lines_indicated_parallel = set()
    for lines_list in symbol_to_closest_line.values():
        for line_id_str in lines_list:
            lines_indicated_parallel.add(line_id_str)
    
    parallel_pairs = []
    for line1_id_str, line2_id_str in combinations(list(lines_indicated_parallel), 2):
        l1_coords = lines_dict[line1_id_str]
        l2_coords = lines_dict[line2_id_str]
        v1 = np.array(l1_coords[1]) - np.array(l1_coords[0])
        v2 = np.array(l2_coords[1]) - np.array(l2_coords[0])
        if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0: continue
        v1_norm, v2_norm = v1 / np.linalg.norm(v1), v2 / np.linalg.norm(v2)

        if abs(abs(np.dot(v1_norm, v2_norm)) - 1) < PARALLELISM_DOT_THRESHOLD: # Check for collinearity (parallel)
            l1_p1_id, l1_p2_id = line1_id_str.split('-')
            l2_p1_id, l2_p2_id = line2_id_str.split('-')
            if all(pid in point_id_to_identifier for pid in [l1_p1_id, l1_p2_id, l2_p1_id, l2_p2_id]):
                l1_p1_ident, l1_p2_ident = point_id_to_identifier[l1_p1_id], point_id_to_identifier[l1_p2_id]
                l2_p1_ident, l2_p2_ident = point_id_to_identifier[l2_p1_id], point_id_to_identifier[l2_p2_id]
                parallel_pairs.append((tuple(sorted((l1_p1_ident, l1_p2_ident))), tuple(sorted((l2_p1_ident, l2_p2_ident)))))

    return {'parallelism': list(set(parallel_pairs))}


# --- Formatting Logic Forms ---
def _format_logic_form_entry(relation_type, elements, value=None):
    if value is not None:
        return f"Equals({relation_type}({elements}), {value})"
    return f"{relation_type}({elements})"

def format_line_instances(annotation):
    point_id_to_identifier = annotation['point_id_to_identifier']
    lines_dict = annotation['lines']
    line_instances = []
    
    for line_id_str, _ in lines_dict.items():
        p1_id, p2_id = line_id_str.split('-')
        if p1_id in point_id_to_identifier and p2_id in point_id_to_identifier:
            p1_ident, p2_ident = point_id_to_identifier[p1_id], point_id_to_identifier[p2_id]
            line_key = "".join(sorted((p1_ident, p2_ident)))
            line_instances.append(line_key)
    
    return {'line_instances': line_instances}

def format_circle_instances(annotation):
    point_id_to_identifier = annotation['point_id_to_identifier']
    circles_dict = annotation['circles']
    circle_instances = []
    
    for c_id, c_data in circles_dict.items():
        cx, cy, radius, _ = c_data
        c_identifier = point_id_to_identifier.get(c_id, c_id)
        circle_instances.append(c_identifier)
    
    return {'circle_instances': circle_instances}

def format_logic_forms(geo_relations):
    logic_forms = []
    # Point on Line Relations
    for point_id, line_key in geo_relations.get('point_on_line', []):
        point_str = point_id if isinstance(point_id, str) else f"Point({point_id})"
        line_str = _format_geo_element_string("Line", line_key)
        logic_forms.append(_format_logic_form_entry("PointLiesOnLine", f"{point_str}, {line_str}"))

    # Point on Circle Relations
    for point_id, circle_id in geo_relations.get('point_on_circle', []):
        point_str = point_id if isinstance(point_id, str) else f"Point({point_id})"
        circle_str = _format_geo_element_string("Circle", circle_id)
        logic_forms.append(_format_logic_form_entry("PointLiesOnCircle", f"{point_str}, {circle_str}"))

    # Assignments
    for line_key, length in geo_relations.get('line_to_length', {}).items():
        line_str = _format_geo_element_string("Line", line_key)
        logic_forms.append(_format_logic_form_entry("LengthOf", line_str, length))
    for angle_key, degree in geo_relations.get('angle_to_degree', {}).items():
        angle_str = _format_geo_element_string("Angle", angle_key)
        logic_forms.append(_format_logic_form_entry("MeasureOf", angle_str, degree))
    for shape_key, area in geo_relations.get('shape_to_area', {}).items(): # shape_key is "Shape(0)" etc.
        logic_forms.append(_format_logic_form_entry("AreaOf", shape_key, area))
    
    # Equalities
    for a1_key, a2_key in geo_relations.get('angle_equations', []):
        a1_str = _format_geo_element_string("Angle", a1_key)
        a2_str = _format_geo_element_string("Angle", a2_key)
        logic_forms.append(f"Equals(MeasureOf({a1_str}), MeasureOf({a2_str}))")
    for l1_key, l2_key in geo_relations.get('length_equations', []):
        l1_str = _format_geo_element_string("Line", l1_key)
        l2_str = _format_geo_element_string("Line", l2_key)
        logic_forms.append(f"Equals(LengthOf({l1_str}), LengthOf({l2_str}))")

    # Properties
    for l1_key, l2_key in geo_relations.get('perpendicularity', []):
        l1_str = _format_geo_element_string("Line", l1_key)
        l2_str = _format_geo_element_string("Line", l2_key)
        logic_forms.append(f"Perpendicular({l1_str}, {l2_str})")
    for l1_key, l2_key in geo_relations.get('parallelism', []):
        l1_str = _format_geo_element_string("Line", l1_key)
        l2_str = _format_geo_element_string("Line", l2_key)
        logic_forms.append(f"Parallel({l1_str}, {l2_str})")
        
    return logic_forms


# --- Plotting ---
def plot_annotation(image_path, annotation, point_id_to_identifier, output_path=None):
    symbols = annotation['symbols']
    labeled_points = set([
        sym['fixed_content'] for sym in symbols.values()
        if sym['type'] == 'text-point'
    ])    
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    fig, ax = plt.subplots(1)
    ax.imshow(image)

    for point_id, coords in annotation['points'].items():
        identifier = point_id_to_identifier.get(point_id, point_id)
        if identifier in labeled_points:
            continue
        default_keywork = {
            'fontsize': 36,
            'color': 'blue',
            'ha': 'left',
            'va': 'bottom',
        }
        # Add the point label to the image
        ax.plot(coords[0], coords[1], 'ro', markersize=8)
        ax.text(coords[0] + 5, coords[1] + 5, identifier, **default_keywork)

    for line_id, (p1_coords, p2_coords) in annotation['lines'].items():
        ax.plot([p1_coords[0], p2_coords[0]], [p1_coords[1], p2_coords[1]], 'b-', linewidth=1.5)

    for circle_center_id, (cx, cy, rad, tol) in annotation['circles'].items():
        ax.add_patch(patches.Circle((cx, cy), rad, color='green', fill=False, linewidth=1.5))

    # for sym_id, sym_info in annotation['symbols'].items():
    #     cx, cy, w, h = sym_info['bbox']
    #     rect_patch = patches.Rectangle((cx - w/2, cy - h/2), w, h, linewidth=1, edgecolor='purple', facecolor='none')
    #     ax.add_patch(rect_patch)
    #     content = sym_info.get('fixed_content', sym_info.get('content', ''))
    #     if content:
    #          ax.text(cx, cy - h/2 - 2, content, va='bottom', ha='center', fontsize=8, color='purple', 
    #                  bbox=dict(facecolor='white', alpha=0.7, pad=0.1, edgecolor='none'))

    plt.axis('off')
    plt.tight_layout()
    if output_path:
        plt.savefig(output_path, bbox_inches='tight', pad_inches=0)
        plt.close(fig)
    else:
        plt.show()



# ---Line Segmentation with UNet---
def transform_image(image, img_size=512):
    if isinstance(image, str):
        image = cv2.imread(image)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    elif isinstance(image, np.ndarray):
        pass  # already in numpy format
    else:
        raise ValueError("Image must be a file path or a numpy array.")

    # Resize and normalize
    transform = A.Compose([
        A.Resize(img_size, img_size),
        A.Normalize(),
        ToTensorV2()
    ])
    
    augmented = transform(image=image)
    return augmented['image']


def segment_lines(model, image, device='cuda'):
    model.eval()
    if not torch.cuda.is_available():
        device = 'cpu'
        
    with torch.no_grad():
        if isinstance(image, str):
            image = cv2.imread(image)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        elif isinstance(image, np.ndarray):
            pass  # already in numpy format
        else:
            raise ValueError("Image must be a file path or a numpy array.")
        
        h_orig, w_orig = image.shape[:2]
        # Transform the image
        image = transform_image(image, img_size=512)
        image_tensor = image.unsqueeze(0).to(device)

        # Predict
        pred = torch.sigmoid(model(image_tensor))
        pred = (pred > 0.5).float().cpu().squeeze().numpy()
    
        # Resize prediction to original image dimensions
        pred = cv2.resize(pred, (w_orig, h_orig), interpolation=cv2.INTER_NEAREST)
        # Convert to gray scale
        pred = (pred * 255).astype(np.uint8)

    return pred


def build_annotation_for_image(
        image_path,
        class_to_index,
        yolo_model,
        unet_model,
        ocr_processor,
        ocr_model,
        output_path=None
    ):
    """
    Build annotations for a single image using YOLO, UNet, and OCR.
    """
    solver_functions = [
        solve_point_on_line_relation,
        solve_point_on_circle_relation,
        solve_assignment_correspondence,
        solve_angle_equality,
        solve_length_equality,
        solve_perpendicularity,
        solve_parallelism
    ]
    mask_image = segment_lines(unet_model, image_path)
    annotation = generate_annotation(yolo_model, image_path, class_to_index)
    annotation = update_lines_with_mask(annotation, mask_image)
    annotation = update_text_content_with_ocr(annotation, image_path, ocr_processor, ocr_model)
    symbol_maps = build_symbol_map(annotation)  # Uses default threshold ratio
    point_id_to_identifier = annotate_keypoints(annotation, symbol_maps)
    annotation['point_id_to_identifier'] = point_id_to_identifier
    annotation['point_positions'] = {
        point_id_to_identifier[p_id]: pos for p_id, pos in annotation['points'].items() if p_id in point_id_to_identifier
    }
    annotation.update(format_line_instances(annotation))
    annotation.update(format_circle_instances(annotation))

    # Plot if requested
    if output_path:
        plot_annotation(image_path, annotation, point_id_to_identifier, output_path)

    # Solve geometric relations
    geo_relations = {}
    for func in solver_functions:
        try:
            relations = func(annotation, symbol_maps, point_id_to_identifier)
            geo_relations.update(relations)
        except Exception as e:
            print(f"Error in solver {func.__name__} for {image_path}: {e}")
    # Format logic forms
    try:
        annotation['diagram_logic_forms'] = format_logic_forms(geo_relations)
    except Exception as e:
        annotation['diagram_logic_forms'] = [f"Error formatting logic forms: {e}"]
        print(f"Error in formatting logic forms for {image_path}: {e}")
    return annotation


# --- Main Orchestration ---
def build_annotations_for_dataset(
        image_dir,
        class_to_index,
        yolo_model, # Pass models
        unet_model,
        ocr_processor,
        ocr_model,
        predication_image_dir=None,
):
    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    annotations_dataset = {}
    

    for image_filename in tqdm.tqdm(image_files, desc="Processing images"):
        pid = os.path.splitext(image_filename)[0]
        image_path = os.path.join(image_dir, image_filename)

        try:
            if predication_image_dir:
                pred_img_path = os.path.join(predication_image_dir, image_filename)

            annotation = build_annotation_for_image(
                image_path=image_path,
                class_to_index=class_to_index,
                yolo_model=yolo_model,
                unet_model=unet_model,
                ocr_processor=ocr_processor,
                ocr_model=ocr_model,
                output_path=pred_img_path if predication_image_dir else None
            )
            annotations_dataset[pid] = annotation

        except Exception as e:
            print(f"Failed to process {image_filename}: {e}")
            annotations_dataset[pid] = {'error': f"Failed to parse the diagram: {str(e)}", 'image': image_filename} # Store error info
            continue
            
    # Sort by integer part of pid if possible
    try:
        annotations_dataset = dict(sorted(annotations_dataset.items(), key=lambda x: int(re.match(r'\d+', x[0]).group(0)) if re.match(r'\d+', x[0]) else x[0] ))
    except ValueError: # Fallback if pids are not purely numeric
        annotations_dataset = dict(sorted(annotations_dataset.items()))
        
    return annotations_dataset


def main():
    parser = argparse.ArgumentParser(description="Generate geometric annotations for images.")
    parser.add_argument("--image_dir", type=str, default="../example", help="Directory containing input images.")
    parser.add_argument("--class_index_file", type=str, default="config/yolo_class_to_index.yaml", help="Path to class_to_index.yaml file.")
    parser.add_argument("--output_dir", type=str, default="annotations", help="Directory to save annotations and prediction images.")
    parser.add_argument("--yolo_model_path", type=str, default="models/best-yolo11n-seg.pt", help="Path to the YOLO model weights.")
    parser.add_argument("--unet_model_path", type=str, default='models/best-unet.pth', help="Path to the UNet model for mask refinement.")
    parser.add_argument("--ocr_model_name", type=str, default="breezedeus/pix2text-mfr", help="Name of the OCR model from HuggingFace.")
    parser.add_argument("--no_plots", action='store_true', help="Disable plotting of prediction images.")
    args = parser.parse_args()

    # Create output directories
    predication_image_dir = os.path.join(args.output_dir, "predication_images")
    if not args.no_plots:
        os.makedirs(predication_image_dir, exist_ok=True)
    else:
        predication_image_dir = None # Pass None to disable plotting

    annotation_output_path = os.path.join(args.output_dir, "diagram_annotations.json")
    os.makedirs(args.output_dir, exist_ok=True)


    # Load class_to_index
    with open(args.class_index_file, 'r') as f:
        class_to_index = yaml.safe_load(f)

    # Load models
    print(f"Loading YOLO model from: {args.yolo_model_path}")
    yolo_model = YOLO(args.yolo_model_path)
    
    print(f"Loading UNet model from: {args.unet_model_path}")
    unet_model = smp.Unet(encoder_name="mobilenet_v2", encoder_weights="imagenet", in_channels=3, classes=1)
    unet_checkpoint = torch.load(args.unet_model_path, map_location='cuda')
    unet_model.load_state_dict(unet_checkpoint)
    unet_model = unet_model.cuda()
    
    print(f"Loading OCR model: {args.ocr_model_name}")
    ocr_processor = TrOCRProcessor.from_pretrained(args.ocr_model_name)
    ocr_model = ORTModelForVision2Seq.from_pretrained(args.ocr_model_name, use_cache=False) # use_cache=False for ONNX models often
    # ocr_processor = TrOCRProcessor.from_pretrained("./models/best-ocr-model")
    # ocr_model = VisionEncoderDecoderModel.from_pretrained("./models/best-ocr-model")

    print("Starting annotation generation...")
    annotations = build_annotations_for_dataset(
        image_dir=args.image_dir,
        class_to_index=class_to_index,
        yolo_model=yolo_model,
        unet_model=unet_model,
        ocr_processor=ocr_processor,
        ocr_model=ocr_model,
        predication_image_dir=predication_image_dir
    )

    # Save the annotations
    print(f"Saving annotations to: {annotation_output_path}")
    with open(annotation_output_path, 'w') as f:
        json.dump(annotations, f, indent=4)
    print("Processing complete.")


if __name__ == "__main__":
    main()